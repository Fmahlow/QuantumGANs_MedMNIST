{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# GANs Clássicas com MedMNIST\n\nEste notebook demonstra como treinar três variantes de GANs utilizando o dataset **MedMNIST**: **DCGAN**, **Conditional GAN** (CGAN) e **Wasserstein GAN com Gradiente Penalty** (WGAN-GP). O objetivo é gerar imagens sintéticas similares às do conjunto de dados."}, {"cell_type": "markdown", "metadata": {}, "source": "## 1. Setup e Importações\nInstale as dependências necessárias e carregue as principais bibliotecas utilizadas ao longo do notebook."}, {"cell_type": "code", "metadata": {}, "source": "!pip install torch torchvision medmnist matplotlib torchmetrics seaborn scikit-learn scipy --quiet\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport medmnist\nfrom medmnist import INFO\nimport matplotlib.pyplot as plt\nimport seaborn as sns", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "## 2. Carregamento e Preparação do Dataset\nUtilizaremos o subset `PathMNIST` por ser relativamente pequeno e adequado para demonstrações rápidas."}, {"cell_type": "code", "metadata": {}, "source": "# Seleciona o dataset\nDATA_FLAG = 'pathmnist'\ninfo = INFO[DATA_FLAG]\ndownload = True\n\n# Transformações básicas: conversão para tensor e normalização\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\n# Carrega treino e teste\ntrain_dataset = getattr(medmnist, info['python_class'])(split='train', transform=transform, download=download)\ntest_dataset = getattr(medmnist, info['python_class'])(split='test', transform=transform, download=download)\n\nbatch_size = 128\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "## 3. Implementações das GANs\nA seguir estão as implementações dos três modelos de GAN."}, {"cell_type": "markdown", "metadata": {}, "source": "### DCGAN"}, {"cell_type": "code", "metadata": {}, "source": "class DCGenerator(nn.Module):\n    def __init__(self, latent_dim=100, img_channels=3):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.ConvTranspose2d(latent_dim, 128, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, img_channels, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\nclass DCDiscriminator(nn.Module):\n    def __init__(self, img_channels=3):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(img_channels, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Flatten(),\n            nn.Linear(128*7*7, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.model(x)", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "### Conditional GAN"}, {"cell_type": "code", "metadata": {}, "source": "class CGANGenerator(nn.Module):\n    def __init__(self, latent_dim=100, num_classes=9, img_channels=3):\n        super().__init__()\n        self.label_emb = nn.Embedding(num_classes, num_classes)\n        self.model = nn.Sequential(\n            nn.ConvTranspose2d(latent_dim + num_classes, 128, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, img_channels, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, noise, labels):\n        label_input = self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n        x = torch.cat([noise, label_input], 1)\n        return self.model(x)\n\nclass CGANDiscriminator(nn.Module):\n    def __init__(self, num_classes=9, img_channels=3):\n        super().__init__()\n        self.label_emb = nn.Embedding(num_classes, num_classes)\n        self.conv = nn.Sequential(\n            nn.Conv2d(img_channels + num_classes, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128*7*7, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, img, labels):\n        label_img = self.label_emb(labels).unsqueeze(2).unsqueeze(3).expand(-1, -1, img.size(2), img.size(3))\n        x = torch.cat([img, label_img], 1)\n        x = self.conv(x)\n        return self.fc(x)", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "### WGAN-GP"}, {"cell_type": "code", "metadata": {}, "source": "class WGANGPGenerator(DCGenerator):\n    pass\n\nclass WGANGPCritic(nn.Module):\n    def __init__(self, img_channels=3):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(img_channels, 64, 4, 2, 1),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Flatten(),\n            nn.Linear(128*7*7, 1)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\ndef gradient_penalty(critic, real, fake):\n    batch_size = real.size(0)\n    epsilon = torch.rand(batch_size, 1, 1, 1, device=real.device)\n    interpolated = epsilon*real + (1-epsilon)*fake\n    interpolated.requires_grad_(True)\n    mixed_scores = critic(interpolated)\n    grad_outputs = torch.ones_like(mixed_scores)\n    gradient = torch.autograd.grad(\n        inputs=interpolated,\n        outputs=mixed_scores,\n        grad_outputs=grad_outputs,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True\n    )[0]\n    gradient = gradient.view(batch_size, -1)\n    gp = ((gradient.norm(2, dim=1) - 1)**2).mean()\n    return gp", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "## 4. Loop de Treinamento\nAqui apresentamos um laço simplificado para treinar cada uma das GANs. Ajuste hiperparâmetros conforme necessário."}, {"cell_type": "code", "metadata": {}, "source": "# Exemplo de treinamento para DCGAN (pode ser adaptado para CGAN e WGAN-GP)\nlatent_dim = 100\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nG = DCGenerator(latent_dim=latent_dim).to(device)\nD = DCDiscriminator().to(device)\n\ncriterion = nn.BCELoss()\noptim_G = torch.optim.Adam(G.parameters(), lr=2e-4, betas=(0.5, 0.999))\noptim_D = torch.optim.Adam(D.parameters(), lr=2e-4, betas=(0.5, 0.999))\n\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    for imgs, labels in train_loader:\n        real = imgs.to(device)\n        b_size = real.size(0)\n        noise = torch.randn(b_size, latent_dim, 1, 1, device=device)\n\n        # Treina Discriminador\n        optim_D.zero_grad()\n        fake = G(noise)\n        loss_real = criterion(D(real), torch.ones(b_size,1,device=device))\n        loss_fake = criterion(D(fake.detach()), torch.zeros(b_size,1,device=device))\n        loss_D = loss_real + loss_fake\n        loss_D.backward()\n        optim_D.step()\n\n        # Treina Gerador\n        optim_G.zero_grad()\n        output = D(fake)\n        loss_G = criterion(output, torch.ones(b_size,1,device=device))\n        loss_G.backward()\n        optim_G.step()\n\n    print(f\"Epoch {epoch+1}/{num_epochs} - Loss D: {loss_D.item():.3f} | Loss G: {loss_G.item():.3f}\")", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "## 5. Avaliação com Métricas\nAvaliamos as imagens geradas utilizando as métricas **FID** e **Inception Score**, disponíveis no pacote `torchmetrics`."}, {"cell_type": "code", "metadata": {}, "source": "from torchmetrics.image.fid import FrechetInceptionDistance\nfrom torchmetrics.image.inception import InceptionScore\n\nfid = FrechetInceptionDistance(feature=64).to(device)\nis_metric = InceptionScore().to(device)\n\n# Exemplo: cálculo das métricas com lotes de imagens reais e geradas\nwith torch.no_grad():\n    for real, _ in train_loader:\n        real = real.to(device)\n        noise = torch.randn(real.size(0), latent_dim,1,1, device=device)\n        fake = G(noise)\n        fid.update(real, real=True)\n        fid.update(fake, real=False)\n        is_metric.update(fake)\n\nfid_score = fid.compute()\nis_mean, is_std = is_metric.compute()\nprint('FID:', fid_score.item())\nprint('IS:', is_mean.item(), '+/-', is_std.item())", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "## 6. Visualização dos Resultados\nMostramos amostras geradas pelos modelos e gráficos de perda."}, {"cell_type": "code", "metadata": {}, "source": "# Geração de amostras\nG.eval()\nnoise = torch.randn(16, latent_dim, 1, 1, device=device)\nwith torch.no_grad():\n    samples = G(noise).cpu()\n\ngrid = torchvision.utils.make_grid(samples, nrow=4, normalize=True)\nplt.figure(figsize=(6,6))\nplt.imshow(grid.permute(1,2,0))\nplt.axis('off')\nplt.show()", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": "## 7. Conclusão\nDiscutimos rapidamente os resultados obtidos e possíveis caminhos para melhorias futuras."}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 2}
